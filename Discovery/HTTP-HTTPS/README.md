# Website scan

**Summery:**

- [Website scan](#website-scan)
  - [Nmap](#nmap)
  - [Scan for cms](#scan-for-cms)
  - [WebSite Scanning](#website-scanning)
    - [Dico list](#dico-list)
    - [Commands](#commands)
    - [Check for backup files](#check-for-backup-files)
  - [403 Bypasser](#403-bypasser)
  - [Fuzzer](#fuzzer)
    - [Fuzz parameters - arjun](#fuzz-parameters---arjun)
    - [Wfuzz](#wfuzz)
  - [HeartBleed](#heartbleed)
    - [Proxy](#proxy)
  - [Zaproxy](#zaproxy)
  - [Check multiple page](#check-multiple-page)
  - [Detect WAF](#detect-waf)
  - [cURL](#curl)
  - [Security.txt](#securitytxt)
  - [Index page](#index-page)
  - [Discovery - Favicon](#discovery---favicon)
  - [OSINT](#osint)
    - [Google dorks](#google-dorks)
    - [Wayback Machine](#wayback-machine)

## Nmap

```sh
nmap -p 80 -vv --script=http-enum.nse,http-methods.nse,http-majordomo2-dir-traversal.nse,http-auth.nse,http-passwd.nse,http-php-version.nse,http-phpmyadmin-dir-traversal.nse,http-put.nse,http-apache-negotiation.nse,http-adobe-coldfusion-apsa1301.nse TARGET_IP
```

## Scan for cms

> Cmsmap

CMSmap is a CMS scanner that automates the process of detecting security flaws of the most popular CMSs.

```sh
cmsmap http://TARGET_IP:PORT
```

> Wpscan

The WPScan CLI tool is a black box WordPress security scanner

```sh
wpscan --url http://TARGET_IP:PORT
```

> Nikto

Nikto is an web server scanner which performs comprehensive tests against web servers for multiple

```sh
nikto -h http://TARGET_IP:PORT
```

> chameleon

```sh
chameleon --url http://TARGET_IP:PORT
```

> Whatweb

WhatWeb recognizes web technologies including content management systems (CMS), blogging platforms, statistic/analytics packages, JavaScript libraries, web servers, and embedded devices.

```sh
whatweb -v -a 3 http://TARGET_IP:PORT
```

## WebSite Scanning

### Dico list

> Common

```list
/usr/share/dirbuster/wordlists/common.txt               - Size 13 KiB
/usr/share/skipfish/dictionaries/complete.wl            - Size 34 KiB
/usr/share/seclists/Discovery/Web-Content/common.txt    - Size 36 KiB
```

> Directory

```list
/usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt   - Size 244 KiB
/usr/share/dirbuster/wordlists/directory-list-2.3-medium.txt            - Size 2 MiB
/usr/share/dirbuster/wordlists/directory-list-2.3-big.txt               - Size 15 MiB
```

> File

```list
/usr/share/seclists/Discovery/Web-Content/raft-medium-files.txt         - 219 KiB
/usr/share/seclists/Discovery/Web-Content/raft-large-files.txt          - 481 KiB
```

> Fuzz extension

```list
/usr/share/seclists/Discovery/Web-Content/web-extensions.txt            - Size 206 B
/usr/share/skipfish/dictionaries/extensions-only.wl                     - Size 1 KiB
/usr/share/seclists/Discovery/Web-Content/raft-small-extensions.txt     - Size 7 KiB
```

> Server Based

```list
/usr/share/seclists/Discovery/Web-Content/IIS.fuzz.txt                  - Size 4 KiB
/usr/share/seclists/Discovery/Web-Content/tomcat.txt                    - Size 2 KiB
/usr/share/seclists/Discovery/Web-Content/nginx.txt                     - Size 559 B
/usr/share/seclists/Discovery/Web-Content/apache.txt                    - Size 238 B
```

> Vhost

```list
/usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt       - Size 32 KiB
```

> Parameters

```list
/usr/share/seclists/Discovery/Web-Content/burp-parameter-names.txt      - Size 18 KiB
```

### Commands

> Skipfish

    skipfish -m 5 -LY -S /usr/share/skipfish/dictionaries/complete.wl -u http://TARGET_IP:Port

> Dirsearch

    dirsearch -u http://TARGET_IP:Port -e php,asp

> Gobuster

    gobuster dir -t 10 -x .php,.html -e -s 200,204,301,302,307 -w /usr/share/seclists/Discovery/Web-Content/raft-large-files.txt -u http://TARGET_IP:Port

> feroxbuster

    feroxbuster --url http://TARGET_IP:Port --depth 2

> Dirhunt

Dirhunt is a web crawler optimize for search and analyze directories. This tool can find interesting things if the server has the "index of" mode enabled.

    dirhunt http://TARGET_IP:Port

### Check for backup files

Source: <https://github.com/mazen160/bfac.git>

	bfac -u http://TARGET_IP:Port

> Exclude status-codes

	bfac -u http://TARGET_IP:Port  --exclude-status-codes 301,999

> Random user agent

    bfac -u http://TARGET_IP:Port -ra

## 403 Bypasser

    403bypasser.py -u http://TARGET_IP:Port -d /DIRECTORY
    403bypasser.py -u http://TARGET_IP:Port -D dirlist.txt

## Fuzzer

### Fuzz parameters - arjun

    arjun -u http://TARGET_IP:Port

> Custom dico

    arjun -u http://TARGET_IP:Port -f /usr/share/seclists/Discovery/Web-Content/burp-parameter-names.txt

### Wfuzz

    wfuzz -c -z file,'/usr/share/wfuzz/wordlist/Injections/All_attack.txt' http://TARGET_IP:Port/index.php?page=FUZZ

> If site in https

    ffuf -w /usr/share/seclists/Discovery/Web-Content/common.txt -u https://TARGET_IP:PORT/index.php?page=FUZZ

## HeartBleed

    sslscan TARGET_IP

    testssl TARGET_IP

    openssl s_client -connect TARGET_IP:443

### Proxy

    Burp

## Zaproxy

    zaproxy

## Check multiple page

The file hosts has multiple different url :
    www.test.com www.google.com ...

    cat hosts |aquatone

## Detect WAF

> nmap

```sh
nmap -p80 --script http-waf-detect TARGET_IP
```

> wafw00f

```sh
wafw00f http://TARGET_IP
wafw00f https://TARGET_IP -v
```

## cURL

> File upload with cURL

```bash
curl -X PUT -d @FILE_TO_UPLOAD.txt http://TARGET_IP/FILE_TO_UPLOAD.txt -vv
```

> json
```bash
curl http://TARGET_IP -H 'Content-Type: application/json' -d '{ "username" : "root", "password" : "root" }'
```

> Cookie file
```bash
curl --cookie-jar cookies.txt http://TARGET_IP
```

> Follow redirection
```bash
curl -L http://TARGET_IP
```

> Only show the header
```bash
curl -I http://TARGET_IP
```

## Security.txt

This file is used to tell security researchers how they can disclose vulnerabilities for a website.  

> Path

```url
http://IP_ADDRESS/.well-known/security.txt
```

> Link

```url
https://securitytxt.org
```

## Index page

```url
http://IP_ADDRESS/robots.txt
http://IP_ADDRESS/sitemap.xml
http://IP_ADDRESS/crossdomain.xml
http://IP_ADDRESS/clientaccesspolicy.xml
```

## Discovery - Favicon

> Favicon database

<https://wiki.owasp.org/index.php/OWASP_favicon_database>

> Get the favicon hash

```bash
curl http://IP_ADDRESS/favicon.ico | md5sum
```

## OSINT

### Google dorks

| Filter   | Example            | Description                                                  |
| :------- | :----------------- | :----------------------------------------------------------- |
| site     | site:tryhackme.com | returns results only from the specified website address      |
| inurl    | inurl:admin        | returns results that have the specified word in the URL      |
| filetype | filetype:pdf       | returns results which are a particular file extension        |
| intitle  | intitle:admin      | returns results that contain the specified word in the title |

More info: <https://en.wikipedia.org/wiki/Google_hacking>

### Wayback Machine

<https://archive.org/web/>
